{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "45584eca-efac-4601-b3f0-4055afef839c",
    "_uuid": "62a2a52e-4e95-4a11-9c08-3893496a6575",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-11-13T14:52:09.881295Z",
     "iopub.status.busy": "2021-11-13T14:52:09.880620Z",
     "iopub.status.idle": "2021-11-13T14:52:17.840077Z",
     "shell.execute_reply": "2021-11-13T14:52:17.838995Z",
     "shell.execute_reply.started": "2021-11-13T14:52:09.881221Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#########################################################\n",
    "# Submission script for the Petfinder.my kaggle contest #\n",
    "#########################################################\n",
    "\n",
    "\n",
    "####################\n",
    "# Python libraries #\n",
    "####################\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import random\n",
    "import imgaug.augmenters as iaa\n",
    "import tensorflow as tf\n",
    "\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "from time import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras.layers import Flatten, Dense, Dropout, Activation,\\\n",
    "    Conv2D, GlobalAveragePooling2D, Lambda, Concatenate, BatchNormalization\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau,\\\n",
    "    EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Model, load_model, save_model\n",
    "from tensorflow.keras.metrics import RootMeanSquaredError\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras import Input\n",
    "\n",
    "tf.random.set_seed(48)\n",
    "\n",
    "# Class importation from kaggle database\n",
    "sys.path.append('../input/swin-transformer-tf')\n",
    "from swintransformer import SwinTransformer\n",
    "\n",
    "\n",
    "#########################\n",
    "# Constants definitions #\n",
    "#########################\n",
    "\n",
    "\n",
    "WEIGHTS = \"/kaggle/input/efficientnet-keras-noisystudent-weights-b0b7\\\n",
    "            /imagenet/imagenet.notop-b0.h5\"\n",
    "INPUT = \"/kaggle/input/petfinder-pawpularity-score/\"\n",
    "TRAIN = \"/kaggle/input/petfinder-pawpularity-score/train\"\n",
    "MODEL = \"/kaggle/input/swin-large-01\"\n",
    "TEST = \"/kaggle/input/petfinder-pawpularity-score/test\"\n",
    "OUT = \"/kaggle/working/\"\n",
    "SIZE = (224, 224)\n",
    "BATCH = 16\n",
    "SEED = 48\n",
    "FEATS = ['ratio', 'pixels', 'Subject Focus', 'Eyes', 'Face', 'Near', 'Action',\n",
    "         'Accessory', 'Group', 'Collage', 'Human', 'Occlusion', 'Info', 'Blur']\n",
    "\n",
    "########################\n",
    "# Processing functions #\n",
    "########################\n",
    "\n",
    "\n",
    "def create_train_df(name, input_dir=INPUT, pics_dir=TRAIN):\n",
    "    \"\"\"\n",
    "    Create reference dataframes for the whole modelization process.\n",
    "    \"\"\"\n",
    "    df_data = pd.read_csv(input_dir + name)\n",
    "    train_pics = glob(pics_dir + \"/*\")\n",
    "\n",
    "    dico = {}\n",
    "\n",
    "    def paw_class_10(x):\n",
    "\n",
    "        if x > 89:\n",
    "            return \"9\"\n",
    "\n",
    "        elif x > 79:\n",
    "            return \"8\"\n",
    "\n",
    "        elif x > 69:\n",
    "            return \"7\"\n",
    "\n",
    "        elif x > 59:\n",
    "            return \"6\"\n",
    "\n",
    "        elif x > 49:\n",
    "            return \"5\"\n",
    "\n",
    "        elif x > 39:\n",
    "            return \"4\"\n",
    "\n",
    "        elif x > 29:\n",
    "            return \"3\"\n",
    "\n",
    "        elif x > 19:\n",
    "            return \"2\"\n",
    "\n",
    "        elif x > 9:\n",
    "            return \"1\"\n",
    "\n",
    "        else:\n",
    "            return \"0\"\n",
    "\n",
    "    def paw_classification(x):\n",
    "\n",
    "        if x > 79:\n",
    "            return \"a\"\n",
    "\n",
    "        elif x > 59:\n",
    "            return \"b\"\n",
    "\n",
    "        elif x > 39:\n",
    "            return \"c\"\n",
    "\n",
    "        elif x > 19:\n",
    "            return \"d\"\n",
    "\n",
    "        else:\n",
    "            return \"e\"\n",
    "\n",
    "    for file in train_pics:\n",
    "\n",
    "        with Image.open(file) as img:\n",
    "\n",
    "            id = file.split(\"/\")[-1].split(\".\")[0]\n",
    "            image_name = file.split(\"/\")[-1]\n",
    "            ratio = max([img.size[0]/img.size[1], img.size[1]/img.size[0]])\n",
    "            dico[id] = (file, image_name, img.size, img.size[0], img.size[1],\n",
    "                        ratio, img.size[0]*img.size[1], img.mode)\n",
    "\n",
    "    df = pd.DataFrame.from_dict(dico, orient='index',\n",
    "                                columns=[\"chemin\", \"nom\", \"taille\",\n",
    "                                         \"longueur\", \"hauteur\",\n",
    "                                         \"ratio\", \"pixels\", \"mode\"])\n",
    "\n",
    "    df = df.rename_axis('Id').reset_index()\n",
    "    df = df.merge(df_data, left_on=\"Id\", right_on=\"Id\")\n",
    "\n",
    "    df[\"classe\"] = df[\"Pawpularity\"].map(paw_classification)\n",
    "    df[\"dizaine\"] = df[\"Pawpularity\"].map(paw_class_10)\n",
    "    df[\"Pawpularity\"] = df[\"Pawpularity\"]/100\n",
    "\n",
    "    max_ratio = df[\"ratio\"].max()\n",
    "    max_pixels = df[\"pixels\"].max()\n",
    "\n",
    "    df[\"ratio\"] = df[\"ratio\"]/max_ratio\n",
    "    df[\"pixels\"] = df[\"pixels\"]/max_pixels\n",
    "\n",
    "    return df, max_ratio, max_pixels\n",
    "\n",
    "\n",
    "def create_test_df(name, input_dir=INPUT, pics_dir=TEST):\n",
    "    \"\"\"\n",
    "    Reference dataframe for the test samples.\n",
    "    \"\"\"\n",
    "    df_data = pd.read_csv(input_dir + name)\n",
    "    train_pics = glob(pics_dir + \"/*\")\n",
    "\n",
    "    dico = {}\n",
    "\n",
    "    for file in train_pics:\n",
    "\n",
    "        with Image.open(file) as img:\n",
    "\n",
    "            id = file.split(\"/\")[-1].split(\".\")[0]\n",
    "            image_name = file.split(\"/\")[-1]\n",
    "            ratio = max([img.size[0]/img.size[1], img.size[1]/img.size[0]])\n",
    "            dico[id] = (file, image_name, img.size, img.size[0], img.size[1],\n",
    "                        ratio, img.size[0]*img.size[1], img.mode)\n",
    "\n",
    "    df = pd.DataFrame.from_dict(dico, orient='index',\n",
    "                                columns=[\"chemin\", \"nom\", \"taille\", \"longueur\",\n",
    "                                         \"hauteur\", \"ratio\", \"pixels\", \"mode\"])\n",
    "\n",
    "    df = df.rename_axis('Id').reset_index()\n",
    "    df = df.merge(df_data, left_on=\"Id\", right_on=\"Id\")\n",
    "\n",
    "    df[\"ratio\"] = df[\"ratio\"]/MAX_RATIO\n",
    "    df[\"pixels\"] = df[\"pixels\"]/MAX_PIXELS\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def weights_data(df):\n",
    "    \"\"\"\n",
    "    Create weights for the data.\n",
    "    \"\"\"\n",
    "    liste_classe = []\n",
    "    dico_classe = {}\n",
    "\n",
    "    for val in sorted(set(df[\"classe\"].values)):\n",
    "\n",
    "        df_temp = df[df[\"classe\"] == val]\n",
    "        weight = 1/len(df_temp)\n",
    "        liste_classe.append(weight)\n",
    "\n",
    "    fact_cl = 1/max(liste_classe)\n",
    "\n",
    "    for val in sorted(set(df[\"classe\"].values)):\n",
    "\n",
    "        df_temp = df[df[\"classe\"] == val]\n",
    "        weight_n = fact_cl/len(df_temp)\n",
    "        dico_classe[val] = weight_n\n",
    "\n",
    "    liste_dizaine = []\n",
    "    dico_dizaine = {}\n",
    "\n",
    "    for val in sorted(set(df[\"dizaine\"].values)):\n",
    "\n",
    "        df_temp = df[df[\"dizaine\"] == val]\n",
    "        weight = 1/len(df_temp)\n",
    "        liste_dizaine.append(weight)\n",
    "\n",
    "    fact_d = 1/max(liste_dizaine)\n",
    "\n",
    "    for val in sorted(set(df[\"dizaine\"].values)):\n",
    "\n",
    "        df_temp = df[df[\"dizaine\"] == val]\n",
    "        weight_n = fact_d/len(df_temp)\n",
    "        dico_dizaine[val] = weight_n\n",
    "\n",
    "    df[\"classe_w\"] = df[\"classe\"].map(dico_classe)\n",
    "    df[\"dizaine_w\"] = df[\"dizaine\"].map(dico_dizaine)\n",
    "\n",
    "\n",
    "def tts_df(df):\n",
    "    \"\"\"\n",
    "    Create balanced train and validation data taking into account\n",
    "    the pawpularity values.\n",
    "    \"\"\"\n",
    "    df_train = pd.DataFrame()\n",
    "    df_val = pd.DataFrame()\n",
    "\n",
    "    for val in sorted(set(df[\"dizaine\"].values)):\n",
    "\n",
    "        df_temp = df[df[\"dizaine\"] == val]\n",
    "        df_temp_train, df_temp_val = train_test_split(df_temp, test_size=0.2)\n",
    "        df_train = pd.concat([df_train, df_temp_train])\n",
    "        df_val = pd.concat([df_val, df_temp_val])\n",
    "\n",
    "    df_train = df_train.sort_values(\"nom\")\n",
    "    df_val = df_val.sort_values(\"nom\")\n",
    "\n",
    "    return df_train, df_val\n",
    "\n",
    "\n",
    "########################\n",
    "# Data generator class #\n",
    "########################\n",
    "\n",
    "class DataGen(Sequence):\n",
    "    \"\"\"\n",
    "    Generator class.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 directory,\n",
    "                 df,\n",
    "                 labels,\n",
    "                 batch_size,\n",
    "                 input_size,\n",
    "                 feat_col,\n",
    "                 iteration=1,\n",
    "                 weight=False,\n",
    "                 training=True,\n",
    "                 aug=True,\n",
    "                 shuffle=True,\n",
    "                 crop=True,\n",
    "                 pp=False,\n",
    "                 pp_func=None):\n",
    "\n",
    "        self.directory = directory\n",
    "        self.df = df.copy()\n",
    "        self.data = pd.DataFrame()\n",
    "        self.batch_size = batch_size\n",
    "        self.input_size = input_size\n",
    "        self.feat_col = feat_col\n",
    "        self.iteration = iteration\n",
    "        self.weight = weight\n",
    "        self.training = training\n",
    "        self.aug = aug\n",
    "\n",
    "        if self.training:\n",
    "\n",
    "            self.labels = labels\n",
    "\n",
    "        self.shuffle = shuffle\n",
    "        self.crop = crop\n",
    "        self.pp = pp\n",
    "        self.pp_func = pp_func\n",
    "        self.zoom = iaa.geometric.Affine(scale=(1, 1.15))\n",
    "        self.crop = iaa.size.CenterCropToSquare()\n",
    "        self.tx = iaa.TranslateX(percent=(-7, 7), mode=\"reflect\")\n",
    "        self.ty = iaa.TranslateY(percent=(-7, 7), mode=\"reflect\")\n",
    "\n",
    "        for i in range(self.iteration):\n",
    "\n",
    "            self.data = pd.concat([self.data, self.df])\n",
    "\n",
    "        self.nb_samples = len(self.data)\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        return int(np.ceil(self.nb_samples / self.batch_size))\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "\n",
    "        if self.shuffle:\n",
    "\n",
    "            self.data = self.data.sample(\n",
    "                frac=1, random_state=SEED).reset_index(drop=True)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        temp_data = self.data.iloc[index*self.batch_size:\n",
    "                                   (index+1)*self.batch_size]\n",
    "        images = []\n",
    "\n",
    "        for name in temp_data[\"nom\"].values:\n",
    "\n",
    "            path = os.path.join(self.directory, name)\n",
    "            img = cv2.imread(path)\n",
    "\n",
    "            if self.crop:\n",
    "\n",
    "                img = self.crop.augment_image(img)\n",
    "\n",
    "            img = cv2.resize(img, SIZE, interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "            if self.aug:\n",
    "\n",
    "                flip_or_not = random.getrandbits(1)\n",
    "\n",
    "                if flip_or_not:\n",
    "\n",
    "                    img = cv2.flip(img, 1)\n",
    "\n",
    "                img = self.zoom.augment_image(img)\n",
    "                img = self.tx.augment_image(img)\n",
    "                img = self.ty.augment_image(img)\n",
    "\n",
    "            if self.pp:\n",
    "\n",
    "                img = self.pp_func(img)\n",
    "\n",
    "            img = np.array(img, dtype='float32')\n",
    "            images.append(img)\n",
    "\n",
    "        if self.training:\n",
    "\n",
    "            if self.weight:\n",
    "\n",
    "                return [np.array(images), temp_data[self.feat_col].values],\\\n",
    "                        temp_data[self.labels].values,\\\n",
    "                        temp_data[\"dizaine_w\"].values\n",
    "\n",
    "            else:\n",
    "\n",
    "                return [np.array(images), temp_data[self.feat_col].values],\\\n",
    "                        temp_data[self.labels].values\n",
    "\n",
    "        else:\n",
    "\n",
    "            return [np.array(images), temp_data[self.feat_col].values]\n",
    "\n",
    "\n",
    "#########################################\n",
    "# Submission and modelization functions #\n",
    "#########################################\n",
    "\n",
    "def submission(model, X_test, df_test):\n",
    "    \"\"\"\n",
    "    Create the submission csv file.\n",
    "    \"\"\"\n",
    "    preds = model.predict(X_test)\n",
    "    df_test[\"Pawpularity\"] = list(np.squeeze(preds))\n",
    "    df_test[\"Pawpularity\"] = df_test[\"Pawpularity\"] * 100\n",
    "    df_test = df_test[[\"Id\", \"Pawpularity\"]]\n",
    "    df_test.to_csv(OUT + \"/\" + \"submission.csv\", index=False)\n",
    "\n",
    "\n",
    "def modelize(model, nb_epochs, train_gen, val_gen, train_steps, val_steps,\n",
    "             batch, mod_name, opti=\"adam\", verbose=1, graph=True):\n",
    "    \"\"\"\n",
    "    Modelization function.\n",
    "    \"\"\"\n",
    "    start = time()\n",
    "\n",
    "    mod_save_path = mod_name\n",
    "\n",
    "    reduce_lr = ReduceLROnPlateau(monitor=\"val_root_mean_squared_error\",\n",
    "                                  patience=1,\n",
    "                                  factor=0.5,\n",
    "                                  verbose=1,\n",
    "                                  mode='min')\n",
    "\n",
    "    stop = EarlyStopping(monitor='val_root_mean_squared_error', patience=4)\n",
    "\n",
    "    check = ModelCheckpoint(mod_save_path,\n",
    "                            save_weights_only=False,\n",
    "                            monitor='val_root_mean_squared_error',\n",
    "                            save_best_only=True)\n",
    "\n",
    "    model.compile(loss=\"binary_crossentropy\",\n",
    "                  optimizer=opti,\n",
    "                  metrics=RootMeanSquaredError())\n",
    "\n",
    "    history = model.fit(train_gen,\n",
    "                        steps_per_epoch=train_steps,\n",
    "                        epochs=nb_epochs,\n",
    "                        batch_size=batch,\n",
    "                        validation_data=val_gen,\n",
    "                        validation_steps=val_steps,\n",
    "                        verbose=verbose,\n",
    "                        callbacks=[check, stop, reduce_lr])\n",
    "\n",
    "    model.save(OUT + mod_name)\n",
    "\n",
    "    train_rmse = history.history['root_mean_squared_error']\n",
    "    val_rmse = history.history['val_root_mean_squared_error']\n",
    "    train_loss = history.history[\"loss\"]\n",
    "    val_loss = history.history[\"val_loss\"]\n",
    "\n",
    "    nb_ep = len(history.history[\"loss\"])\n",
    "\n",
    "    if graph:\n",
    "\n",
    "        fig = plt.figure(figsize=(11, 9))\n",
    "\n",
    "        plt.subplot(2, 1, 1)\n",
    "\n",
    "        plt.plot(range(1, nb_ep+1),\n",
    "                 train_loss,\n",
    "                 'b',\n",
    "                 label='Train loss',\n",
    "                 color=\"green\")\n",
    "\n",
    "        plt.plot(range(1, nb_ep+1),\n",
    "                 val_loss,\n",
    "                 'b',\n",
    "                 label='Validation loss',\n",
    "                 color=\"yellow\")\n",
    "\n",
    "        plt.title('Train & validation losses - ' + mod_name)\n",
    "        plt.xlabel(\"Epochs\")\n",
    "        plt.ylabel(\"Losses\")\n",
    "        plt.legend()\n",
    "\n",
    "        plt.subplot(2, 1, 2)\n",
    "\n",
    "        plt.plot(range(1, nb_ep+1),\n",
    "                 train_rmse,\n",
    "                 'b',\n",
    "                 label='Train RMSE',\n",
    "                 color=\"red\")\n",
    "\n",
    "        plt.plot(range(1, nb_ep+1),\n",
    "                 val_rmse,\n",
    "                 'b',\n",
    "                 label='Validation RMSE',\n",
    "                 color=\"blue\")\n",
    "\n",
    "        plt.title('Train & validation RMSE - ' + mod_name)\n",
    "        plt.xlabel(\"Epochs\")\n",
    "        plt.ylabel(\"Accuracy\")\n",
    "        plt.legend()\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "    print()\n",
    "    print(f\"Entrainement pour {nb_ep} epochs : {time()-start:.0f} secondes.\")\n",
    "    print(f\"Meilleure train_RMSE = {np.min(train_rmse):.3f}\")\n",
    "    print(f\"Meilleure val_RMSE = {np.min(val_rmse):.3f}\")\n",
    "    print()\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "#####################################################################\n",
    "# Model creation function.                                          #\n",
    "# To deal with the \"no internet\" kaggle competition rule, the model #\n",
    "# is not created here but imported as an \"added data\".              #\n",
    "#####################################################################\n",
    "\n",
    "def create_swin(mod):\n",
    "\n",
    "    inputs = Input(shape=(224, 224, 3))\n",
    "    fx = Lambda(lambda data: tf.keras.applications.imagenet_utils.\n",
    "                preprocess_input(tf.cast(data, tf.float32),\n",
    "                                 mode=\"torch\"))(inputs)\n",
    "    fx = mod(fx)\n",
    "    fx = Dropout(0.2)(fx)\n",
    "\n",
    "    meta = Input(14)\n",
    "\n",
    "    x = Concatenate()([fx, meta])\n",
    "\n",
    "    x = Dense(128, activation=\"relu\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Dense(64, activation=\"relu\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "\n",
    "    output = Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "    model = Model(inputs=[inputs, meta], outputs=output)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "##########\n",
    "# Script #\n",
    "##########\n",
    "\n",
    "# Reference dataframes creation\n",
    "df, MAX_RATIO, MAX_PIXELS = create_train_df(\"train.csv\")\n",
    "weights_data(df)\n",
    "df_train, df_val = tts_df(df)\n",
    "\n",
    "df_test = create_test_df(\"test.csv\")\n",
    "\n",
    "# Train, validation and test generators\n",
    "train_gen = DataGen(TRAIN,\n",
    "                    df_train,\n",
    "                    \"Pawpularity\",\n",
    "                    BATCH,\n",
    "                    SIZE,\n",
    "                    FEATS,\n",
    "                    2,  # Iterations\n",
    "                    False,  # weight\n",
    "                    True,  # training\n",
    "                    True,  # aug\n",
    "                    True,  # shuffle\n",
    "                    True,  # crop\n",
    "                    False,  # use prepro funct\n",
    "                    None)  # prepro func name\n",
    "\n",
    "val_gen = DataGen(TRAIN,\n",
    "                  df_val,\n",
    "                  \"Pawpularity\",\n",
    "                  BATCH,\n",
    "                  SIZE,\n",
    "                  FEATS,\n",
    "                  1,\n",
    "                  False,  # weight\n",
    "                  True,  # training\n",
    "                  False,  # aug\n",
    "                  True,  # shuffle\n",
    "                  True,  # crop\n",
    "                  False,  # use prepro func\n",
    "                  None)  # prepro func name\n",
    "\n",
    "train_steps, val_steps = len(train_gen), len(val_gen)\n",
    "\n",
    "test_gen = DataGen(TEST,\n",
    "                   df_test,\n",
    "                   \"Pawpularity\",\n",
    "                   1,\n",
    "                   SIZE,\n",
    "                   FEATS,\n",
    "                   1,  # Iteration\n",
    "                   False,  # weight\n",
    "                   False,  # training\n",
    "                   False,  # aug\n",
    "                   False,  # shuffle\n",
    "                   True,  # crop\n",
    "                   False,  # use prepro func\n",
    "                   None)  # prepro func name\n",
    "\n",
    "# If the kaggle rules allowed it, we would here instanciate\n",
    "# a SWIN model and pass it in our \"create_swin\" function.\n",
    "# Then we would freeze it to just train the classification layers.\n",
    "#\n",
    "# swin = SwinTransformer(\"swin_large_224\", include_top=False,\n",
    "#                        pretrained=True, use_tpu=False)\n",
    "#\n",
    "# model = create_swin(swin)\n",
    "# for layer in model.layers[:4]:\n",
    "#\n",
    "#    layer.trainable = False\n",
    "\n",
    "# But because of the \"no internet\" rule, we just load our complete model\n",
    "# from an added database...\n",
    "model = load_model(MODEL)\n",
    "\n",
    "# Model training\n",
    "train_model = modelize(model, 30, train_gen, val_gen, train_steps,\n",
    "                       val_steps, BATCH, \"large_swin_extractor\",\n",
    "                       opti=\"adam\", verbose=1, graph=False)\n",
    "\n",
    "# Results submission\n",
    "submission(train_model, test_gen, df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
